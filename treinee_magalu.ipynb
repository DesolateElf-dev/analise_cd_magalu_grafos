{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a198cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5406090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ufs_nordeste = ['AL', 'BA', 'CE', 'MA', 'PB', 'PE', 'PI', 'RN', 'SE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cfdb97",
   "metadata": {},
   "source": [
    "# Tratando PIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d708d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pib = pd.read_excel('PIB dos Munic√≠pios - base de dados 2010-2021.xlsx')\n",
    "#print(df_pib_pc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pib = df_pib[df_pib['Ano'] == 2021][[\n",
    "    'Sigla da Unidade da Federa√ß√£o',\n",
    "    'Nome da Unidade da Federa√ß√£o',\n",
    "    'Nome do Munic√≠pio',\n",
    "    'Produto Interno Bruto, \\na pre√ßos correntes\\n(R$ 1.000)',\n",
    "    'Produto Interno Bruto per capita, \\na pre√ßos correntes\\n(R$ 1,00)'\n",
    "]]\n",
    "\n",
    "df_pib = df_pib[df_pib['Sigla da Unidade da Federa√ß√£o'].isin(ufs_nordeste)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a1af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pib = df_pib.rename(columns={\n",
    "    'Sigla da Unidade da Federa√ß√£o': 'sigla_UF',\n",
    "    'Nome da Unidade da Federa√ß√£o': 'nome_UF',\n",
    "    'Nome do Munic√≠pio': 'nome_municipio',\n",
    "    'Produto Interno Bruto, \\na pre√ßos correntes\\n(R$ 1.000)': 'PIB',\n",
    "    'Produto Interno Bruto per capita, \\na pre√ßos correntes\\n(R$ 1,00)': 'PIB_pc'\n",
    "})\n",
    "df_pib = df_pib.sort_values(by='PIB', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995a0e2d",
   "metadata": {},
   "source": [
    "# Tratando rotas e PIB csv Recife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba618baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recife = pd.read_csv('rotas_regic2021_brasil (Recife).csv')\n",
    "#print(df_para_recife.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283d445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recife = df_recife.drop(columns=['modal', 'cod_o', 'cod_uf_d'])\n",
    "#print(df_recife.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4313cf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recife = df_recife[df_recife['cod_uf_o'].isin(ufs_nordeste)]\n",
    "#print(df_nordeste.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc52c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recife = df_recife.rename(columns={\n",
    "    'cod_uf_o': 'sigla_UF',\n",
    "    'nome_d': 'nome_UF',\n",
    "    'nome_o': 'nome_municipio',\n",
    "    'Produto Interno Bruto, \\na pre√ßos correntes\\n(R$ 1.000)': 'PIB',\n",
    "    'Produto Interno Bruto per capita, \\na pre√ßos correntes\\n(R$ 1,00)': 'PIB_pc'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01776cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_final_recife = df_recife.merge(\n",
    "    df_pib_pc[['Nome do Munic√≠pio', 'PIB']],\n",
    "    left_on='nome_o',\n",
    "    right_on='Nome do Munic√≠pio',\n",
    "    how='left'\n",
    ")\n",
    "df_final_recife = df_final_recife.drop(columns=['Nome do Munic√≠pio'])\n",
    "df_final_recife = df_final_recife.sort_values(by='PIB', ascending=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d729ffb",
   "metadata": {},
   "source": [
    "# Tratando rotas e PIB csv Salvador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salvador = pd.read_csv('rotas_regic2021_brasil (salvador).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309468e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salvador = df_salvador.drop(columns=['modal', 'cod_o', 'cod_uf_d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salvador = df_salvador[df_salvador['cod_uf_o'].isin(ufs_nordeste)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b1acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salvador = df_salvador.rename(columns={\n",
    "    'cod_uf_o': 'sigla_UF',\n",
    "    'nome_d': 'nome_UF',\n",
    "    'nome_o': 'nome_municipio',\n",
    "    'Produto Interno Bruto, \\na pre√ßos correntes\\n(R$ 1.000)': 'PIB',\n",
    "    'Produto Interno Bruto per capita, \\na pre√ßos correntes\\n(R$ 1,00)': 'PIB_pc'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_final_salvador = df_salvador.merge(\n",
    "    df_pib_pc[['Nome do Munic√≠pio', 'PIB']],\n",
    "    left_on='nome_o',\n",
    "    right_on='Nome do Munic√≠pio',\n",
    "    how='left'\n",
    ")\n",
    "df_final_salvador = df_final_salvador.drop(columns=['Nome do Munic√≠pio'])\n",
    "df_final_salvador = df_final_salvador.sort_values(by='PIB', ascending=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e559b2d",
   "metadata": {},
   "source": [
    "# Grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. AN√ÅLISE DE GRAFOS - Criando rede de distribui√ß√£o\n",
    "print(\"=== 1. AN√ÅLISE DE GRAFOS ===\")\n",
    "\n",
    "# Criar grafo da malha log√≠stica\n",
    "G = nx.Graph()\n",
    "\n",
    "# Adicionar n√≥s (cidades) com atributos\n",
    "for i, row in df_pib.iterrows():\n",
    "    G.add_node(row['nome_municipio'], pib=row['PIB'])\n",
    "\n",
    "for _, row in df_recife.iterrows():\n",
    "    G.add_edge('Recife', row['nome_municipio'], weight=row['km'])  # peso num√©rico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ad3a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pos = nx.spring_layout(G, seed=42)  # layout por mola\n",
    "nx.draw_networkx(G, pos, node_size=400, font_size=8)  # n√≥s e r√≥tulos\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45aeb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pyvis\n",
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(height=\"600px\", width=\"100%\", notebook=True, directed=False)\n",
    "net.from_nx(G)\n",
    "net.show(\"rede.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ed61b0",
   "metadata": {},
   "source": [
    "# Extra√ß√£o Dist√¢ncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d518425",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "API_KEY = \"eyJvcmciOiI1YjNjZTM1OTc4NTExMTAwMDFjZjYyNDgiLCJpZCI6ImM3ZGEzZWU1ZWZhZDQyYzhiMTE2OWIzMDk4MDliMDBmIiwiaCI6Im11cm11cjY0In0=\"\n",
    "URL_ROUTE = \"https://api.openrouteservice.org/v2/directions/driving-car\"\n",
    "URL_GEOCODE = \"https://api.openrouteservice.org/geocode/search\"\n",
    "\n",
    "# Carrega lista de cidades\n",
    "df_cidades = pd.read_csv(\"lista_cidades.csv\")  # ou pd.read_excel(\"lista_cidades.xlsx\")\n",
    "cidades = df_cidades[\"Cidade\"].tolist()\n",
    "\n",
    "resultados = []\n",
    "backup_respostas = []\n",
    "\n",
    "# Se j√° existir checkpoint, carregar progresso\n",
    "if os.path.exists(\"checkpoint_resultados.json\"):\n",
    "    with open(\"checkpoint_resultados.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        checkpoint = json.load(f)\n",
    "        resultados = checkpoint[\"resultados\"]\n",
    "        backup_respostas = checkpoint[\"backup\"]\n",
    "        cidades_processadas = {(item[\"origem\"], item[\"destino\"]) for item in backup_respostas}\n",
    "        print(f\"‚ñ∂ Retomando execu√ß√£o. {len(cidades_processadas)} rotas j√° processadas.\")\n",
    "else:\n",
    "    cidades_processadas = set()\n",
    "\n",
    "def geocode(texto):\n",
    "    \"\"\"Retorna coordenadas (lon, lat) de um endere√ßo\"\"\"\n",
    "    r = requests.get(URL_GEOCODE, params={\"api_key\": API_KEY, \"text\": texto})\n",
    "    data = r.json()\n",
    "    if \"features\" in data and len(data[\"features\"]) > 0:\n",
    "        return data[\"features\"][0][\"geometry\"][\"coordinates\"]\n",
    "    return None\n",
    "\n",
    "def consulta_rota(origem, destino):\n",
    "    \"\"\"Consulta rota e retorna resposta completa + resumo\"\"\"\n",
    "    origem_coord = geocode(origem)\n",
    "    destino_coord = geocode(destino)\n",
    "\n",
    "    if not origem_coord or not destino_coord:\n",
    "        return None, None\n",
    "\n",
    "    body = {\"coordinates\": [origem_coord, destino_coord]}\n",
    "    headers = {\"Authorization\": API_KEY, \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    r = requests.post(URL_ROUTE, json=body, headers=headers)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Erro: {origem} -> {destino}: {r.text}\")\n",
    "        return None, None\n",
    "\n",
    "    data = r.json()\n",
    "    try:\n",
    "        summary = data[\"features\"][0][\"properties\"][\"summary\"]\n",
    "        distancia_km = summary[\"distance\"] / 1000\n",
    "        duracao_h = summary[\"duration\"] / 3600\n",
    "    except (KeyError, IndexError):\n",
    "        return data, None\n",
    "\n",
    "    resumo = {\n",
    "        \"origem\": origem,\n",
    "        \"destino\": destino,\n",
    "        \"distancia_km\": round(distancia_km, 1),\n",
    "        \"duracao_h\": round(duracao_h, 2),\n",
    "    }\n",
    "    return data, resumo\n",
    "\n",
    "contador = 0\n",
    "for cidade in cidades:\n",
    "    for origem in [\"Recife, PE\", \"Salvador, BA\"]:\n",
    "        if (origem, cidade) in cidades_processadas:\n",
    "            continue\n",
    "\n",
    "        data, resumo = consulta_rota(origem, cidade)\n",
    "        if data:\n",
    "            backup_respostas.append({\"origem\": origem, \"destino\": cidade, \"resposta_completa\": data})\n",
    "        if resumo:\n",
    "            resultados.append(resumo)\n",
    "\n",
    "        contador += 1\n",
    "        # Salvar checkpoint a cada 100 rotas\n",
    "        if contador % 100 == 0:\n",
    "            with open(\"checkpoint_resultados.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\"resultados\": resultados, \"backup\": backup_respostas}, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"üíæ Checkpoint salvo ({contador} rotas).\")\n",
    "\n",
    "        time.sleep(1)  # respeita rate limit\n",
    "\n",
    "# Salvar resultado final\n",
    "with open(\"respostas_completas.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(backup_respostas, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "df_resultados.to_excel(\"distancias_resumo.xlsx\", index=False)\n",
    "\n",
    "# Remover checkpoint ap√≥s concluir\n",
    "if os.path.exists(\"checkpoint_resultados.json\"):\n",
    "    os.remove(\"checkpoint_resultados.json\")\n",
    "\n",
    "print(\"‚úÖ Arquivos finais gerados: respostas_completas.json e distancias_resumo.xlsx\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f37777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Par√¢metros\n",
    "API_KEY = \"eyJvcmciOiI1YjNjZTM1OTc4NTExMTAwMDFjZjYyNDgiLCJpZCI6ImM3ZGEzZWU1ZWZhZDQyYzhiMTE2OWIzMDk4MDliMDBmIiwiaCI6Im11cm11cjY0In0=\" #os.getenv(\"ORS_API_KEY\")\n",
    "print(API_KEY)\n",
    "PROFILE = \"driving-hgv\"  # ou \"driving-car\"\n",
    "URL_ROUTE = f\"https://api.openrouteservice.org/v2/directions/{PROFILE}/geojson\"\n",
    "\n",
    "INPUT_FILE = \"lista_cidades.csv\"  # deve conter colunas: Cidade, lon, lat, PIB\n",
    "OUT_BACKUP = \"respostas_completas.json\"\n",
    "OUT_SUMMARY = \"distancias_resumo.xlsx\"\n",
    "\n",
    "ORIGENS = [\"Recife\", \"Salvador\"]\n",
    "PIB_LIMIAR = 300000  # R$ 300 mil\n",
    "DAILY_CAP = 1900      # margem abaixo de 2.000\n",
    "SLEEP_S = 1.6         # ~40/min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8e5b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carrega dados\n",
    "if INPUT_FILE.lower().endswith(\".xlsx\"):\n",
    "    df_cidades = pd.read_excel(INPUT_FILE)\n",
    "else:\n",
    "    df_cidades = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# Normaliza nomes\n",
    "df_cidades[\"Cidade\"] = df_cidades[\"Cidade\"].astype(str).str.strip()\n",
    "\n",
    "#corrige moeda\n",
    "df_cidades['PIB'] = (\n",
    "    df_cidades['PIB']\n",
    "    .astype(str)\n",
    "    .str.replace('$', '', regex=False)\n",
    "    .str.replace('.', '', regex=False)   # remove separador de milhar\n",
    "    .str.replace(',', '.', regex=False)  # troca v√≠rgula por ponto decimal\n",
    "    .str.strip()\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "# Valida colunas\n",
    "for col in [\"Cidade\", \"lon\", \"lat\", \"PIB\"]:\n",
    "    if col not in df_cidades.columns:\n",
    "        raise ValueError(f\"Coluna obrigat√≥ria ausente: {col}\")\n",
    "\n",
    "# Filtra apenas as cidades acima do limiar de PIB (mantendo somente as 534 desejadas)\n",
    "df_filtrado = df_cidades[df_cidades[\"PIB\"] > PIB_LIMIAR].copy()\n",
    "\n",
    "# Mapa nome -> [lon, lat]\n",
    "coords = {r[\"Cidade\"]: [float(r[\"lon\"]), float(r[\"lat\"])] for _, r in df_cidades.iterrows()}\n",
    "\n",
    "# Garante que as origens est√£o presentes na tabela\n",
    "for o in ORIGENS:\n",
    "    if o not in coords:\n",
    "        raise ValueError(f\"Origem ausente na tabela: {o}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ff7e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resultados = []\n",
    "backup = []\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\"Authorization\": API_KEY, \"Content-Type\": \"application/json\"})\n",
    "\n",
    "def consulta_rota(origem_nome, destino_nome):\n",
    "    o = coords.get(origem_nome)\n",
    "    d = coords.get(destino_nome)\n",
    "    if not o or not d:\n",
    "        return None, None\n",
    "    body = {\"coordinates\": [o, d]}\n",
    "    r = session.post(URL_ROUTE, json=body, timeout=60)\n",
    "    # Rea√ß√£o simples a 429: aguarda e tenta 1x\n",
    "    if r.status_code == 429:\n",
    "        time.sleep(10)\n",
    "        r = session.post(URL_ROUTE, json=body, timeout=60)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"[ERRO {r.status_code}] {origem_nome} -> {destino_nome}: {r.text[:200]}\")\n",
    "        return None, None\n",
    "    data = r.json()\n",
    "    try:\n",
    "        feat0 = data[\"features\"]\n",
    "        props = feat0[\"properties\"]\n",
    "        summary = props[\"summary\"]\n",
    "        dist_km = float(summary[\"distance\"]) / 1000.0\n",
    "        dur_h = float(summary[\"duration\"]) / 3600.0\n",
    "        resumo = {\n",
    "            \"origem\": origem_nome,\n",
    "            \"destino\": destino_nome,\n",
    "            \"origem_lon\": o, \"origem_lat\": o[21],\n",
    "            \"destino_lon\": d, \"destino_lat\": d[21],\n",
    "            \"perfil\": PROFILE,\n",
    "            \"distancia_km\": round(dist_km, 1),\n",
    "            \"duracao_h\": round(dur_h, 2),\n",
    "        }\n",
    "    except (KeyError, IndexError, TypeError, ValueError):\n",
    "        resumo = None\n",
    "    return data, resumo\n",
    "\n",
    "contador = 0\n",
    "for _, row in df_filtrado.iterrows():\n",
    "    cidade = row[\"Cidade\"]\n",
    "    for origem in ORIGENS:\n",
    "        if contador >= DAILY_CAP:\n",
    "            print(f\"[INFO] Limite di√°rio interno atingido ({DAILY_CAP}). Encerrando.\")\n",
    "            break\n",
    "        data, resumo = consulta_rota(origem, cidade)\n",
    "        if data:\n",
    "            backup.append({\"origem\": origem, \"destino\": cidade, \"resposta_completa\": data})\n",
    "        if resumo:\n",
    "            resultados.append(resumo)\n",
    "        contador += 1\n",
    "        time.sleep(SLEEP_S)\n",
    "    if contador >= DAILY_CAP:\n",
    "        break\n",
    "\n",
    "# Salva resultados\n",
    "with open(OUT_BACKUP, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(backup, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "pd.DataFrame(resultados).to_excel(OUT_SUMMARY, index=False)\n",
    "\n",
    "print(f\"‚úÖ Feito. Rotas: {len(resultados)} | Arquivos: {OUT_BACKUP}, {OUT_SUMMARY}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cea467",
   "metadata": {},
   "source": [
    "## Read .json backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa972b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "OUT_BACKUP = \"raw_data/respostas_completas.json\"\n",
    "\n",
    "with open(OUT_BACKUP, \"r\", encoding=\"utf-8\") as f:\n",
    "    dados = json.load(f)\n",
    "\n",
    "resumo_list = []\n",
    "for item in dados:\n",
    "    origem = item.get(\"origem\")\n",
    "    destino = item.get(\"destino\")\n",
    "    resposta = item.get(\"resposta_completa\", {})\n",
    "    try:\n",
    "        summary = resposta[\"features\"][0][\"properties\"][\"summary\"]\n",
    "        coordinates = resposta[\"metadata\"][\"query\"][\"coordinates\"]\n",
    "        lat = coordinates[1][1]\n",
    "        lon = coordinates[1][0]\n",
    "        distancia_km = summary[\"distance\"] / 1000\n",
    "        duracao_h = summary[\"duration\"] / 3600\n",
    "    except (KeyError, IndexError, TypeError):\n",
    "        distancia_km = None\n",
    "        duracao_h = None\n",
    "    resumo_list.append({\n",
    "        \"origem\": origem,\n",
    "        \"destino\": destino,\n",
    "        \"origem_lat\": lat,\n",
    "        \"origem_lon\": lon,\n",
    "        \"distancia_km\": distancia_km,\n",
    "        \"duracao_h\": duracao_h\n",
    "    })\n",
    "\n",
    "df_resumo = pd.DataFrame(resumo_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f9694",
   "metadata": {},
   "source": [
    "# Limpeza Municipios e contru√ß√£o tabela final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df38126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas xlrd\n",
    "import pandas as pd\n",
    "\n",
    "# Caminho do arquivo .xls (use raw string para evitar problemas com backslashes no Windows)\n",
    "caminho = r\"RELATORIO_DTB_BRASIL_2024_MUNICIPIOS.xls\"\n",
    "nome_planilha = \"DTB_Munic√≠pios\"  # equivalente ao item [Name=\"DTB_Munic√≠pios\"] no Power Query\n",
    "\n",
    "# 1) Ler a planilha sem cabe√ßalho (equivalente a ter Column1..Column17) e como texto inicialmente\n",
    "#    Para .xls, o engine 'xlrd' pode ser necess√°rio (instale xlrd se n√£o estiver presente)\n",
    "df_cidades = pd.read_excel(\n",
    "    caminho,\n",
    "    sheet_name=nome_planilha,\n",
    "    header=None,\n",
    "    dtype=str,\n",
    "    engine=\"xlrd\"  # remova ou ajuste se n√£o for necess√°rio no seu ambiente\n",
    ")\n",
    "\n",
    "# 2) Remover as 6 primeiras linhas (Table.Skip(...,6))\n",
    "df_cidades = df_cidades.iloc[6:].reset_index(drop=True)\n",
    "\n",
    "# 3) Promover a primeira linha a cabe√ßalho (Table.PromoteHeaders)\n",
    "df_cidades.columns = df_cidades.iloc[0].astype(str).str.strip()\n",
    "df_cidades = df_cidades.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "# 4) Definir os tipos conforme o Power Query\n",
    "#    Inteiros (usando dtype pandas 'Int64' que aceita NA) e textos (string Pandas)\n",
    "colunas_inteiras = [\n",
    "    \"UF\",\n",
    "    \"Regi√£o Geogr√°fica Intermedi√°ria\",\n",
    "    \"Nome Regi√£o Geogr√°fica Intermedi√°ria\",  # esta fica texto; listada aqui s√≥ para refer√™ncia\n",
    "    \"Regi√£o Geogr√°fica Imediata\",\n",
    "    \"Nome Regi√£o Geogr√°fica Imediata\",       # esta fica texto; listada aqui s√≥ para refer√™ncia\n",
    "    \"Munic√≠pio\",\n",
    "    \"C√≥digo Munic√≠pio Completo\",\n",
    "]\n",
    "# Converter explicitamente apenas as que s√£o num√©ricas\n",
    "cols_num = [\"UF\", \"Regi√£o Geogr√°fica Intermedi√°ria\", \"Regi√£o Geogr√°fica Imediata\", \"Munic√≠pio\", \"C√≥digo Munic√≠pio Completo\"]\n",
    "for c in cols_num:\n",
    "    if c in df_cidades.columns:\n",
    "        df_cidades[c] = pd.to_numeric(df_cidades[c], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Colunas de texto relevantes\n",
    "cols_texto = [\n",
    "    \"Nome_UF\",\n",
    "    \"Nome Regi√£o Geogr√°fica Intermedi√°ria\",\n",
    "    \"Nome Regi√£o Geogr√°fica Imediata\",\n",
    "    \"Nome_Munic√≠pio\",\n",
    "]\n",
    "for c in cols_texto:\n",
    "    if c in df_cidades.columns:\n",
    "        df_cidades[c] = df_cidades[c].astype(\"string\")\n",
    "\n",
    "# 5) Selecionar somente as colunas desejadas (Table.SelectColumns do M)\n",
    "colunas_desejadas = [\n",
    "    \"UF\",\n",
    "    \"Nome_UF\",\n",
    "    \"Regi√£o Geogr√°fica Intermedi√°ria\",\n",
    "    \"Nome Regi√£o Geogr√°fica Intermedi√°ria\",\n",
    "    \"Regi√£o Geogr√°fica Imediata\",\n",
    "    \"Nome Regi√£o Geogr√°fica Imediata\",\n",
    "    \"Munic√≠pio\",\n",
    "    \"C√≥digo Munic√≠pio Completo\",\n",
    "    \"Nome_Munic√≠pio\",\n",
    "]\n",
    "df_cidades = df_cidades[[c for c in colunas_desejadas if c in df_cidades.columns]]\n",
    "\n",
    "# 6) Remover as colunas de regi√µes (Table.RemoveColumns do M)\n",
    "colunas_remover = [\n",
    "    \"Regi√£o Geogr√°fica Intermedi√°ria\",\n",
    "    \"Nome Regi√£o Geogr√°fica Intermedi√°ria\",\n",
    "    \"Regi√£o Geogr√°fica Imediata\",\n",
    "    \"Nome Regi√£o Geogr√°fica Imediata\",\n",
    "]\n",
    "df_cidades = df_cidades.drop(columns=[c for c in colunas_remover if c in df_cidades.columns])\n",
    "\n",
    "# 7) Remover duplicatas (Table.Distinct)\n",
    "df_cidades = df_cidades.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# 8) Filtrar apenas estados do Nordeste (Table.SelectRows com OR em Nome_UF)\n",
    "estados_ne = [\n",
    "    \"Alagoas\", \"Bahia\", \"Cear√°\", \"Maranh√£o\", \"Para√≠ba\",\n",
    "    \"Pernambuco\", \"Piau√≠\", \"Rio Grande do Norte\", \"Sergipe\"\n",
    "]\n",
    "if \"Nome_UF\" in df_cidades.columns:\n",
    "    df_cidades = df_cidades[df_cidades[\"Nome_UF\"].isin(estados_ne)].reset_index(drop=True)\n",
    "\n",
    "# 9) (Opcional) Exportar o resultado\n",
    "# df.to_csv(\"DTB_Municipios_NE_2024.csv\", index=False)\n",
    "\n",
    "# Para inspe√ß√£o r√°pida:\n",
    "print(df_cidades.head())\n",
    "print(df_cidades.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286604b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Arquivos de entrada (no mesmo diret√≥rio)\n",
    "coords = pd.read_csv('coordenadas_e_pib_cidades.csv')\n",
    "dist = pd.read_csv('distancia_e_tempo_cidades.csv')\n",
    "\n",
    "# Normaliza strings\n",
    "for col in ['origem','destino']:\n",
    "    dist[col] = dist[col].astype(str).str.strip()\n",
    "coords['Cidade'] = coords['Cidade'].astype(str).str.strip()\n",
    "\n",
    "# Converte PIB \"$73436128,432\" -> 73436128.432 (float)\n",
    "def parse_pib(x):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    s = str(x).strip()\n",
    "    s = s.replace('\"','').replace(\"'\", '')\n",
    "    s = s.replace('$','').replace(' ','')\n",
    "    s = s.replace('.', '')       # remove separador de milhar (se houver)\n",
    "    s = s.replace(',', '.')      # v√≠rgula -> ponto\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "coords['PIB'] = coords['PIB'].apply(parse_pib)\n",
    "\n",
    "# Zera self-distances ausentes (origem == destino)\n",
    "mask_self = (dist['origem'] == dist['destino'])\n",
    "dist.loc[mask_self & dist['distancia_km'].isna(), 'distancia_km'] = 0.0\n",
    "dist.loc[mask_self & dist['duracao_h'].isna(), 'duracao_h'] = 0.0\n",
    "\n",
    "# Converte dura√ß√£o para minutos\n",
    "dist['duracao_min'] = pd.to_numeric(dist['duracao_h'], errors='coerce') * 60\n",
    "\n",
    "# Tabela Recife\n",
    "recife = dist[dist['origem'] == 'Recife'][['destino','distancia_km','duracao_min']].rename(\n",
    "    columns={'destino':'Cidade','distancia_km':'dist_recife_km','duracao_min':'tempo_recife_min'}\n",
    ")\n",
    "\n",
    "# Tabela Salvador\n",
    "salvador = dist[dist['origem'] == 'Salvador'][['destino','distancia_km','duracao_min']].rename(\n",
    "    columns={'destino':'Cidade','distancia_km':'dist_salvador_km','duracao_min':'tempo_salvador_min'}\n",
    ")\n",
    "\n",
    "# Merge final\n",
    "merged = coords.merge(recife, on='Cidade', how='left').merge(salvador, on='Cidade', how='left')\n",
    "\n",
    "# Renomeia e organiza colunas conforme o template\n",
    "merged = merged.rename(columns={'Cidade':'municipio', 'PIB':'pib'})\n",
    "\n",
    "# Adiciona coluna 'estado' buscando pelo munic√≠pio no arquivo cod_cidades.csv\n",
    "df_cod = pd.read_csv('cod_cidades.csv')\n",
    "df_cod['Nome_Munic√≠pio'] = df_cod['Nome_Munic√≠pio'].astype(str).str.strip()\n",
    "df_cod['Nome_UF'] = df_cod['Nome_UF'].astype(str).str.strip()\n",
    "\n",
    "# Faz o merge para obter o estado correspondente ao munic√≠pio\n",
    "merged = merged.merge(df_cod[['Nome_Munic√≠pio', 'Nome_UF']], left_on='municipio', right_on='Nome_Munic√≠pio', how='left')\n",
    "merged['estado'] = merged['Nome_UF']\n",
    "merged = merged.drop(columns=['Nome_Munic√≠pio', 'Nome_UF'])\n",
    "\n",
    "cols = ['municipio','estado','lat','lon','pib',\n",
    "        'dist_recife_km','dist_salvador_km','tempo_recife_min','tempo_salvador_min']\n",
    "merged = merged[cols]\n",
    "\n",
    "# Arredonda e salva\n",
    "num_cols = ['lat','lon','pib','dist_recife_km','dist_salvador_km','tempo_recife_min','tempo_salvador_min']\n",
    "merged[num_cols] = merged[num_cols].round(6)\n",
    "\n",
    "\n",
    "\n",
    "# Filtra apenas cidades com PIB > 300.000\n",
    "merged = merged[merged['pib'] > 300000].copy()\n",
    "\n",
    "# Remove linhas com dados faltantes nas colunas de dist√¢ncia\n",
    "merged = merged.dropna(subset=['dist_recife_km', 'dist_salvador_km'])\n",
    "\n",
    "merged.to_csv('dados_nordeste_merged.csv', index=False)\n",
    "print('Arquivo gerado: dados_nordeste_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5086b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dados_nordeste = pd.read_csv('dados_nordeste_merged.csv')\n",
    "\n",
    "df_dados_nordeste.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
